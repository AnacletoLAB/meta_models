<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>meta_models.meta_layers.regularized_meta_layer API documentation</title>
<meta name="description" content="Class implementing meta-model for a Dense Layer." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>meta_models.meta_layers.regularized_meta_layer</code></h1>
</header>
<section id="section-intro">
<p>Class implementing meta-model for a Dense Layer.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Class implementing meta-model for a Dense Layer.&#34;&#34;&#34;
from collections import ChainMap
from typing import Dict, List

import numpy as np
from tensorflow.keras import regularizers

from .meta_layer import MetaLayer


class RegularizedMetaLayer(MetaLayer):
    &#34;&#34;&#34;Class implementing RegularizedMetaLayer.

    The class offers methods for handling the hyper-parameters
    related to l1 and l2 normalization (activity, kernel and bias) and offers
    the optional parameters to enable each of them singularly, other than
    additionally the batch normalization and the dropout.

    Private members
    -------------------------------
    _min_l1_regularization: float,
        Minimum value for L1 regularization.
    _max_l1_regularization: float,
        Maximum value for L1 regularization.
    _min_l2_regularization: float,
        Minimum value for L2 regularization.
    _max_l2_regularization: float,
        Maximum value for L2 regulatization,
    _min_dropout_rate: float,
        Minimum value for dropout rate.
    _max_dropout_rate: float,
        Maximum value for dropout rate.
    _batch_normalization: bool,
        Wethever to use a batch normalization.
    _activity_regularizer: bool,
        Wethever to use an activity regularizer.
    _kernel_regularizer: bool,
        Wethever to use an kernel regularizer.
    _bias_regularizer: bool,
        Wethever to use an bias regularizer.
    _dropout: bool,
        Wethever to use the dropout layer.
    &#34;&#34;&#34;

    regularizers_types = (&#34;l1&#34;, &#34;l2&#34;)

    def __init__(
        self,
        min_l1_regularization: float = 0,
        max_l1_regularization: float = 0.01,
        min_l2_regularization: float = 0,
        max_l2_regularization: float = 0.01,
        min_dropout_rate: float = 0,
        max_dropout_rate: float = 0.5,
        batch_normalization: bool = False,
        activity_regularizer: bool = False,
        kernel_regularizer: bool = False,
        bias_regularizer: bool = False,
        dropout: bool = False,
        **kwargs: Dict
    ):
        &#34;&#34;&#34;Create new DenseResidualLayer meta-model object.

        Parameters
        ----------------------
        min_l1_regularization: float = 0,
            Minimum value of l1 regularization.
            If the tuning process passes 0, then the regularization is skipped.
            This is the minimum value used for all form of regularization,
            but diffent keyword arguments are used depending on what is
            enabled in this layer, so different values will be passed by the
            optimization process. Rarily the regularization layers have vastly
            different values, hence the absence of multiple parameters.
        max_l1_regularization: float = 0.01,
            Maximum value of l1 regularization.
            This is the maximum value used for all form of regularization,
            but diffent keyword arguments are used depending on what is
            enabled in this layer, so different values will be passed by the
            optimization process. Rarily the regularization layers have vastly
            different values, hence the absence of multiple parameters.
        min_l2_regularization: float = 0,
            Minimum value of l2 regularization.
            If the tuning process passes 0, then the regularization is skipped.
            This is the minimum value used for all form of regularization,
            but diffent keyword arguments are used depending on what is
            enabled in this layer, so different values will be passed by the
            optimization process. Rarily the regularization layers have vastly
            different values, hence the absence of multiple parameters.
        max_l2_regularization: float = 0.01,
            Maximum value of l2 regularization.
            This is the maximum value used for all form of regularization,
            but diffent keyword arguments are used depending on what is
            enabled in this layer, so different values will be passed by the
            optimization process. Rarily the regularization layers have vastly
            different values, hence the absence of multiple parameters.
        min_dropout_rate: float = 0,
            Minimum value of dropout.
            If the tuning process passes 0, then the dropout layer is skipped.
        max_dropout_rate: float = 0.5,
            Maximum value of dropout.
            If the tuning process passes 0, then the dropout layer is skipped.
        batch_normalization: bool = False,
            Wethever to use or not batch normalization.
        activity_regularizer: bool = False,
            Wethever to use an activity regularizer.
        kernel_regularizer: bool = False,
            Wethever to use a kernel regularizer.
        bias_regularizer: bool = False,
            Wethever to use a bias regularizer.
        dropout: bool = False,
            Wethever to use a dropout.
        **kwargs: Dict,
            Dictionary of keyword parameters to be passed to parent class.
        &#34;&#34;&#34;
        super().__init__(**kwargs)
        self._min_l1_regularization = min_l1_regularization
        self._max_l1_regularization = max_l1_regularization
        self._min_l2_regularization = min_l2_regularization
        self._max_l2_regularization = max_l2_regularization
        self._min_dropout_rate = min_dropout_rate
        self._max_dropout_rate = max_dropout_rate
        self._batch_normalization = batch_normalization
        self._activity_regularizer = activity_regularizer
        self._kernel_regularizer = kernel_regularizer
        self._bias_regularizer = bias_regularizer
        self._dropout = dropout

    def _active_regularizer_types(self, regularizer: str) -&gt; List[str]:
        &#34;&#34;&#34;Return active regularizers.&#34;&#34;&#34;
        return [
            &#34;{}_{}&#34;.format(regularizer, reg_type)
            for reg_type in RegularizedMetaLayer.regularizers_types
        ]

    def _active_regularizers(self,) -&gt; List[str]:
        &#34;&#34;&#34;Return active regularizers.&#34;&#34;&#34;
        return [
            regularizer
            for regularizer, enabled in zip(
                (
                    &#34;activity_regularizer&#34;,
                    &#34;kernel_regularizer&#34;,
                    &#34;bias_regularizer&#34;
                ),
                (
                    self._activity_regularizer,
                    self._kernel_regularizer,
                    self._bias_regularizer
                )
            )
            if enabled
        ]

    def _space(self) -&gt; Dict:
        &#34;&#34;&#34;Return hyper parameters of the layer.&#34;&#34;&#34;
        return ChainMap(*[
            {
                regularizer_type: reg_range
                for regularizer_type, reg_range in zip(
                    self._active_regularizer_types(regularizer),
                    (
                        (
                            self._min_l1_regularization,
                            self._max_l1_regularization
                        ),
                        (
                            self._min_l2_regularization,
                            self._max_l2_regularization
                        )
                    )
                )
            }
            for regularizer in self._active_regularizers()
        ], {
            &#34;dropout_rate&#34;: (self._min_dropout_rate, self._max_dropout_rate)
        } if self._dropout else {})

    def _build_regularizers(self, **kwargs: Dict) -&gt; Dict:
        &#34;&#34;&#34;Return regularizer for the current layer.

        Parameters
        -------------------------
        **kwargs:Dict,
            keyword arguments for the method, including, optionally:

            kernel_regularizer_l1: int,
                Weight to use for L1 kernel regularization.
            kernel_regularizer_l2: int,
                Weight to use for L2 kernel regularization.
            bias_regularizer_l1: int,
                Weight to use for L1 bias regularization.
            bias_regularizer_l2: int,
                Weight to use for L2 bias regularization.
            activity_regularizer_l1: int,
                Weight to use for L1 activity regularization.
            activity_regularizer_l2: int,
                Weight to use for L2 activity regularization.

        Returns
        ------------------------
        The regularizers keyword arguments and objects for the layer.
        &#34;&#34;&#34;
        return {
            regularizer: regularizers.l1_l2(**{
                regularizer_type: kwargs[name]
                for regularizer_type, name in zip(
                    RegularizedMetaLayer.regularizers_types,
                    self._active_regularizer_types(regularizer)
                )
                if not np.isclose(kwargs[name], 0)
            })
            for regularizer in self._active_regularizers()
        }</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="meta_models.meta_layers.regularized_meta_layer.RegularizedMetaLayer"><code class="flex name class">
<span>class <span class="ident">RegularizedMetaLayer</span></span>
<span>(</span><span>min_l1_regularization: float = 0, max_l1_regularization: float = 0.01, min_l2_regularization: float = 0, max_l2_regularization: float = 0.01, min_dropout_rate: float = 0, max_dropout_rate: float = 0.5, batch_normalization: bool = False, activity_regularizer: bool = False, kernel_regularizer: bool = False, bias_regularizer: bool = False, dropout: bool = False, **kwargs: Dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Class implementing RegularizedMetaLayer.</p>
<p>The class offers methods for handling the hyper-parameters
related to l1 and l2 normalization (activity, kernel and bias) and offers
the optional parameters to enable each of them singularly, other than
additionally the batch normalization and the dropout.</p>
<h2 id="private-members">Private Members</h2>
<p>_min_l1_regularization: float,
Minimum value for L1 regularization.
_max_l1_regularization: float,
Maximum value for L1 regularization.
_min_l2_regularization: float,
Minimum value for L2 regularization.
_max_l2_regularization: float,
Maximum value for L2 regulatization,
_min_dropout_rate: float,
Minimum value for dropout rate.
_max_dropout_rate: float,
Maximum value for dropout rate.
_batch_normalization: bool,
Wethever to use a batch normalization.
_activity_regularizer: bool,
Wethever to use an activity regularizer.
_kernel_regularizer: bool,
Wethever to use an kernel regularizer.
_bias_regularizer: bool,
Wethever to use an bias regularizer.
_dropout: bool,
Wethever to use the dropout layer.</p>
<p>Create new DenseResidualLayer meta-model object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>min_l1_regularization</code></strong> :&ensp;<code>float = 0,</code></dt>
<dd>Minimum value of l1 regularization.
If the tuning process passes 0, then the regularization is skipped.
This is the minimum value used for all form of regularization,
but diffent keyword arguments are used depending on what is
enabled in this layer, so different values will be passed by the
optimization process. Rarily the regularization layers have vastly
different values, hence the absence of multiple parameters.</dd>
<dt><strong><code>max_l1_regularization</code></strong> :&ensp;<code>float = 0.01,</code></dt>
<dd>Maximum value of l1 regularization.
This is the maximum value used for all form of regularization,
but diffent keyword arguments are used depending on what is
enabled in this layer, so different values will be passed by the
optimization process. Rarily the regularization layers have vastly
different values, hence the absence of multiple parameters.</dd>
<dt><strong><code>min_l2_regularization</code></strong> :&ensp;<code>float = 0,</code></dt>
<dd>Minimum value of l2 regularization.
If the tuning process passes 0, then the regularization is skipped.
This is the minimum value used for all form of regularization,
but diffent keyword arguments are used depending on what is
enabled in this layer, so different values will be passed by the
optimization process. Rarily the regularization layers have vastly
different values, hence the absence of multiple parameters.</dd>
<dt><strong><code>max_l2_regularization</code></strong> :&ensp;<code>float = 0.01,</code></dt>
<dd>Maximum value of l2 regularization.
This is the maximum value used for all form of regularization,
but diffent keyword arguments are used depending on what is
enabled in this layer, so different values will be passed by the
optimization process. Rarily the regularization layers have vastly
different values, hence the absence of multiple parameters.</dd>
<dt><strong><code>min_dropout_rate</code></strong> :&ensp;<code>float = 0,</code></dt>
<dd>Minimum value of dropout.
If the tuning process passes 0, then the dropout layer is skipped.</dd>
<dt><strong><code>max_dropout_rate</code></strong> :&ensp;<code>float = 0.5,</code></dt>
<dd>Maximum value of dropout.
If the tuning process passes 0, then the dropout layer is skipped.</dd>
<dt><strong><code>batch_normalization</code></strong> :&ensp;<code>bool = False,</code></dt>
<dd>Wethever to use or not batch normalization.</dd>
<dt><strong><code>activity_regularizer</code></strong> :&ensp;<code>bool = False,</code></dt>
<dd>Wethever to use an activity regularizer.</dd>
<dt><strong><code>kernel_regularizer</code></strong> :&ensp;<code>bool = False,</code></dt>
<dd>Wethever to use a kernel regularizer.</dd>
<dt><strong><code>bias_regularizer</code></strong> :&ensp;<code>bool = False,</code></dt>
<dd>Wethever to use a bias regularizer.</dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>bool = False,</code></dt>
<dd>Wethever to use a dropout.</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>Dict,</code></dt>
<dd>Dictionary of keyword parameters to be passed to parent class.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RegularizedMetaLayer(MetaLayer):
    &#34;&#34;&#34;Class implementing RegularizedMetaLayer.

    The class offers methods for handling the hyper-parameters
    related to l1 and l2 normalization (activity, kernel and bias) and offers
    the optional parameters to enable each of them singularly, other than
    additionally the batch normalization and the dropout.

    Private members
    -------------------------------
    _min_l1_regularization: float,
        Minimum value for L1 regularization.
    _max_l1_regularization: float,
        Maximum value for L1 regularization.
    _min_l2_regularization: float,
        Minimum value for L2 regularization.
    _max_l2_regularization: float,
        Maximum value for L2 regulatization,
    _min_dropout_rate: float,
        Minimum value for dropout rate.
    _max_dropout_rate: float,
        Maximum value for dropout rate.
    _batch_normalization: bool,
        Wethever to use a batch normalization.
    _activity_regularizer: bool,
        Wethever to use an activity regularizer.
    _kernel_regularizer: bool,
        Wethever to use an kernel regularizer.
    _bias_regularizer: bool,
        Wethever to use an bias regularizer.
    _dropout: bool,
        Wethever to use the dropout layer.
    &#34;&#34;&#34;

    regularizers_types = (&#34;l1&#34;, &#34;l2&#34;)

    def __init__(
        self,
        min_l1_regularization: float = 0,
        max_l1_regularization: float = 0.01,
        min_l2_regularization: float = 0,
        max_l2_regularization: float = 0.01,
        min_dropout_rate: float = 0,
        max_dropout_rate: float = 0.5,
        batch_normalization: bool = False,
        activity_regularizer: bool = False,
        kernel_regularizer: bool = False,
        bias_regularizer: bool = False,
        dropout: bool = False,
        **kwargs: Dict
    ):
        &#34;&#34;&#34;Create new DenseResidualLayer meta-model object.

        Parameters
        ----------------------
        min_l1_regularization: float = 0,
            Minimum value of l1 regularization.
            If the tuning process passes 0, then the regularization is skipped.
            This is the minimum value used for all form of regularization,
            but diffent keyword arguments are used depending on what is
            enabled in this layer, so different values will be passed by the
            optimization process. Rarily the regularization layers have vastly
            different values, hence the absence of multiple parameters.
        max_l1_regularization: float = 0.01,
            Maximum value of l1 regularization.
            This is the maximum value used for all form of regularization,
            but diffent keyword arguments are used depending on what is
            enabled in this layer, so different values will be passed by the
            optimization process. Rarily the regularization layers have vastly
            different values, hence the absence of multiple parameters.
        min_l2_regularization: float = 0,
            Minimum value of l2 regularization.
            If the tuning process passes 0, then the regularization is skipped.
            This is the minimum value used for all form of regularization,
            but diffent keyword arguments are used depending on what is
            enabled in this layer, so different values will be passed by the
            optimization process. Rarily the regularization layers have vastly
            different values, hence the absence of multiple parameters.
        max_l2_regularization: float = 0.01,
            Maximum value of l2 regularization.
            This is the maximum value used for all form of regularization,
            but diffent keyword arguments are used depending on what is
            enabled in this layer, so different values will be passed by the
            optimization process. Rarily the regularization layers have vastly
            different values, hence the absence of multiple parameters.
        min_dropout_rate: float = 0,
            Minimum value of dropout.
            If the tuning process passes 0, then the dropout layer is skipped.
        max_dropout_rate: float = 0.5,
            Maximum value of dropout.
            If the tuning process passes 0, then the dropout layer is skipped.
        batch_normalization: bool = False,
            Wethever to use or not batch normalization.
        activity_regularizer: bool = False,
            Wethever to use an activity regularizer.
        kernel_regularizer: bool = False,
            Wethever to use a kernel regularizer.
        bias_regularizer: bool = False,
            Wethever to use a bias regularizer.
        dropout: bool = False,
            Wethever to use a dropout.
        **kwargs: Dict,
            Dictionary of keyword parameters to be passed to parent class.
        &#34;&#34;&#34;
        super().__init__(**kwargs)
        self._min_l1_regularization = min_l1_regularization
        self._max_l1_regularization = max_l1_regularization
        self._min_l2_regularization = min_l2_regularization
        self._max_l2_regularization = max_l2_regularization
        self._min_dropout_rate = min_dropout_rate
        self._max_dropout_rate = max_dropout_rate
        self._batch_normalization = batch_normalization
        self._activity_regularizer = activity_regularizer
        self._kernel_regularizer = kernel_regularizer
        self._bias_regularizer = bias_regularizer
        self._dropout = dropout

    def _active_regularizer_types(self, regularizer: str) -&gt; List[str]:
        &#34;&#34;&#34;Return active regularizers.&#34;&#34;&#34;
        return [
            &#34;{}_{}&#34;.format(regularizer, reg_type)
            for reg_type in RegularizedMetaLayer.regularizers_types
        ]

    def _active_regularizers(self,) -&gt; List[str]:
        &#34;&#34;&#34;Return active regularizers.&#34;&#34;&#34;
        return [
            regularizer
            for regularizer, enabled in zip(
                (
                    &#34;activity_regularizer&#34;,
                    &#34;kernel_regularizer&#34;,
                    &#34;bias_regularizer&#34;
                ),
                (
                    self._activity_regularizer,
                    self._kernel_regularizer,
                    self._bias_regularizer
                )
            )
            if enabled
        ]

    def _space(self) -&gt; Dict:
        &#34;&#34;&#34;Return hyper parameters of the layer.&#34;&#34;&#34;
        return ChainMap(*[
            {
                regularizer_type: reg_range
                for regularizer_type, reg_range in zip(
                    self._active_regularizer_types(regularizer),
                    (
                        (
                            self._min_l1_regularization,
                            self._max_l1_regularization
                        ),
                        (
                            self._min_l2_regularization,
                            self._max_l2_regularization
                        )
                    )
                )
            }
            for regularizer in self._active_regularizers()
        ], {
            &#34;dropout_rate&#34;: (self._min_dropout_rate, self._max_dropout_rate)
        } if self._dropout else {})

    def _build_regularizers(self, **kwargs: Dict) -&gt; Dict:
        &#34;&#34;&#34;Return regularizer for the current layer.

        Parameters
        -------------------------
        **kwargs:Dict,
            keyword arguments for the method, including, optionally:

            kernel_regularizer_l1: int,
                Weight to use for L1 kernel regularization.
            kernel_regularizer_l2: int,
                Weight to use for L2 kernel regularization.
            bias_regularizer_l1: int,
                Weight to use for L1 bias regularization.
            bias_regularizer_l2: int,
                Weight to use for L2 bias regularization.
            activity_regularizer_l1: int,
                Weight to use for L1 activity regularization.
            activity_regularizer_l2: int,
                Weight to use for L2 activity regularization.

        Returns
        ------------------------
        The regularizers keyword arguments and objects for the layer.
        &#34;&#34;&#34;
        return {
            regularizer: regularizers.l1_l2(**{
                regularizer_type: kwargs[name]
                for regularizer_type, name in zip(
                    RegularizedMetaLayer.regularizers_types,
                    self._active_regularizer_types(regularizer)
                )
                if not np.isclose(kwargs[name], 0)
            })
            for regularizer in self._active_regularizers()
        }</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="meta_models.meta_layers.meta_layer.MetaLayer" href="meta_layer.html#meta_models.meta_layers.meta_layer.MetaLayer">MetaLayer</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="meta_models.meta_layers.conv1d_meta_layer.Conv1DMetaLayer" href="conv1d_meta_layer.html#meta_models.meta_layers.conv1d_meta_layer.Conv1DMetaLayer">Conv1DMetaLayer</a></li>
<li><a title="meta_models.meta_layers.conv2d_meta_layer.Conv2DMetaLayer" href="conv2d_meta_layer.html#meta_models.meta_layers.conv2d_meta_layer.Conv2DMetaLayer">Conv2DMetaLayer</a></li>
<li><a title="meta_models.meta_layers.conv3d_meta_layer.Conv3DMetaLayer" href="conv3d_meta_layer.html#meta_models.meta_layers.conv3d_meta_layer.Conv3DMetaLayer">Conv3DMetaLayer</a></li>
<li><a title="meta_models.meta_layers.dense_meta_layer.DenseMetaLayer" href="dense_meta_layer.html#meta_models.meta_layers.dense_meta_layer.DenseMetaLayer">DenseMetaLayer</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="meta_models.meta_layers.regularized_meta_layer.RegularizedMetaLayer.regularizers_types"><code class="name">var <span class="ident">regularizers_types</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="meta_models.meta_layers.meta_layer.MetaLayer" href="meta_layer.html#meta_models.meta_layers.meta_layer.MetaLayer">MetaLayer</a></b></code>:
<ul class="hlist">
<li><code><a title="meta_models.meta_layers.meta_layer.MetaLayer.build" href="meta_layer.html#meta_models.meta_layers.meta_layer.MetaLayer.build">build</a></code></li>
<li><code><a title="meta_models.meta_layers.meta_layer.MetaLayer.layer_prefix" href="meta_layer.html#meta_models.meta_layers.meta_layer.MetaLayer.layer_prefix">layer_prefix</a></code></li>
<li><code><a title="meta_models.meta_layers.meta_layer.MetaLayer.reset" href="meta_layer.html#meta_models.meta_layers.meta_layer.MetaLayer.reset">reset</a></code></li>
<li><code><a title="meta_models.meta_layers.meta_layer.MetaLayer.space" href="meta_layer.html#meta_models.meta_layers.meta_layer.MetaLayer.space">space</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="meta_models.meta_layers" href="index.html">meta_models.meta_layers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="meta_models.meta_layers.regularized_meta_layer.RegularizedMetaLayer" href="#meta_models.meta_layers.regularized_meta_layer.RegularizedMetaLayer">RegularizedMetaLayer</a></code></h4>
<ul class="">
<li><code><a title="meta_models.meta_layers.regularized_meta_layer.RegularizedMetaLayer.regularizers_types" href="#meta_models.meta_layers.regularized_meta_layer.RegularizedMetaLayer.regularizers_types">regularizers_types</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>